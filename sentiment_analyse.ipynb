{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentimentanalyse mit VADER\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) ist ein Tool für Sentimentanalyse, das besonders auf Texte aus den sozialen Medien verwendet wird. Die Sentimentanalyse basiert auf einem manuell annotierten englischen Lexikon (inkl. Emojis, Abkürzungen, Umgangssprache). ([Quelle](https://github.com/cjhutto/vaderSentiment)) \n",
    "\n",
    "Hier testen wir VADER auf [deutsche Kaffeebewertungen](https://www.kaggle.com/mldado/german-online-reviewsratings-of-organic-coffee). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/lorena/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd   #für Datenanalyse\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>brand</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>459</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Ich bevorzuge Gepa Kaffee, da er FairTrade pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>460</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Kaffee genießen mit gutem Gefühl: besonders zu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>461</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Ich habe einen magenfreundlichen, aber koffein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>462</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Bei Gepa unterstützt man bei jedem Kauf ein we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>463</td>\n",
       "      <td>GEPA Kaffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Schon lange gehört Kaffee von Gepa zum festen ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        brand  rating  \\\n",
       "459         459  GEPA Kaffee       5   \n",
       "460         460  GEPA Kaffee       5   \n",
       "461         461  GEPA Kaffee       5   \n",
       "462         462  GEPA Kaffee       5   \n",
       "463         463  GEPA Kaffee       5   \n",
       "\n",
       "                                                review  \n",
       "459  Ich bevorzuge Gepa Kaffee, da er FairTrade pro...  \n",
       "460  Kaffee genießen mit gutem Gefühl: besonders zu...  \n",
       "461  Ich habe einen magenfreundlichen, aber koffein...  \n",
       "462  Bei Gepa unterstützt man bei jedem Kauf ein we...  \n",
       "463  Schon lange gehört Kaffee von Gepa zum festen ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bewertungen = pd.read_csv(\"kaffee_reviews.csv\")\n",
    "bewertungen.tail() # zeigt die letzten 5 Zeilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Wenn ich Bohnenkaffee trinke (auf Arbeit trink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Für mich ist dieser Kaffee ideal. Die Grundvor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Ich persönlich bin insbesondere von dem Geschm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>ganz abgesehen vom geschmack legt gepa inzwisc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Seit Jahren kaufe ich am liebsten den Kaffee u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       5  Wenn ich Bohnenkaffee trinke (auf Arbeit trink...\n",
       "1       5  Für mich ist dieser Kaffee ideal. Die Grundvor...\n",
       "2       5  Ich persönlich bin insbesondere von dem Geschm...\n",
       "3       5  ganz abgesehen vom geschmack legt gepa inzwisc...\n",
       "4       5  Seit Jahren kaufe ich am liebsten den Kaffee u..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# die Spalten \"Unnamed\" und \"brand\" entfernen, weil wir sie für unsere Analyse nicht brauchen\n",
    "\n",
    "kaffee_bewertungen = bewertungen.drop([\"Unnamed: 0\", \"brand\"], axis=1)\n",
    "kaffee_bewertungen.head() # zeigt die ersten 5 Zeilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    346\n",
       "4     87\n",
       "1     23\n",
       "3      8\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# die Anzahl der Bewertungen im jeweiligen Rating anzeigen\n",
    "\n",
    "kaffee_bewertungen.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus den 464 Bewertungen im Dataset, 433 sind positiv (Ratings 4 und 5), 23 sind sehr negativ (Rating 1) und 8 sind neutral (Rating 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda x:\n",
    "\n",
    "kaffee_bewertungen[\"neg\"] = kaffee_bewertungen[\"review\"].apply(lambda x:sia.polarity_scores(x)[\"neg\"])\n",
    "kaffee_bewertungen[\"pos\"] = kaffee_bewertungen[\"review\"].apply(lambda x:sia.polarity_scores(x)[\"pos\"])\n",
    "kaffee_bewertungen[\"neu\"] = kaffee_bewertungen[\"review\"].apply(lambda x:sia.polarity_scores(x)[\"neu\"])\n",
    "kaffee_bewertungen[\"compound\"] = kaffee_bewertungen[\"review\"].apply(lambda x:sia.polarity_scores(x)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>neu</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>Naturland gibt auf ihrer Internetseite an, das...</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.773</td>\n",
       "      <td>-0.9485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>Ich nutze von REWE Bio Röstkaffee Fairtrade di...</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.755</td>\n",
       "      <td>-0.4389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>Naturland von Rewe</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1</td>\n",
       "      <td>…denn auf der Website wird ausführlich erläute...</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.865</td>\n",
       "      <td>-0.9485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>Aus einigen Testberichten verschiedener Intern...</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.9136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>5</td>\n",
       "      <td>Wir haben in der Vergangenheit den SCHIRMER bi...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5</td>\n",
       "      <td>Bio drauf und bio drin, genau so wie es sein s...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>5</td>\n",
       "      <td>das ist in sehr Geschmackvoller Kaffee,den ich...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>5</td>\n",
       "      <td>Kundenbetreuung: 5 Sterne</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>5</td>\n",
       "      <td>Schon lange gehört Kaffee von Gepa zum festen ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.7351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>464 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review    neg    pos  \\\n",
       "38        1  Naturland gibt auf ihrer Internetseite an, das...  0.227  0.000   \n",
       "36        1  Ich nutze von REWE Bio Röstkaffee Fairtrade di...  0.139  0.106   \n",
       "37        1                                 Naturland von Rewe  0.000  0.000   \n",
       "75        1  …denn auf der Website wird ausführlich erläute...  0.135  0.000   \n",
       "102       1  Aus einigen Testberichten verschiedener Intern...  0.206  0.000   \n",
       "..      ...                                                ...    ...    ...   \n",
       "165       5  Wir haben in der Vergangenheit den SCHIRMER bi...  0.000  0.032   \n",
       "164       5  Bio drauf und bio drin, genau so wie es sein s...  0.000  0.000   \n",
       "163       5  das ist in sehr Geschmackvoller Kaffee,den ich...  0.000  0.078   \n",
       "137       5                          Kundenbetreuung: 5 Sterne  0.000  0.000   \n",
       "463       5  Schon lange gehört Kaffee von Gepa zum festen ...  0.000  0.126   \n",
       "\n",
       "       neu  compound  \n",
       "38   0.773   -0.9485  \n",
       "36   0.755   -0.4389  \n",
       "37   1.000    0.0000  \n",
       "75   0.865   -0.9485  \n",
       "102  0.794   -0.9136  \n",
       "..     ...       ...  \n",
       "165  0.968    0.3182  \n",
       "164  1.000    0.0000  \n",
       "163  0.922    0.3182  \n",
       "137  1.000    0.0000  \n",
       "463  0.874    0.7351  \n",
       "\n",
       "[464 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# die Ergebnisse darstellen und in aufsteigender Reihenfolge nach Rating sortieren,\n",
    "# damit wir die Sentiment Werte mit den extremen Ratings (1 und 5) vergleichen können.\n",
    "\n",
    "kaffee_bewertungen.sort_values(by=\"rating\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Werte **'neg'**, **'pos'** und **'neu'** geben an, wie negativ, positiv bzw. neutral eine Bewertung ist. Die Werte liegen zwischen 0 und 1 und die Summe der drei Werte soll 1 (100%) ergeben. Der Wert **'compound'** ergibt sich aus der Summe aller Bewertungen eines Wortes aus dem VADER Lexikon, die zwischen -1 (negativ) und +1 (positiv) normalisiert werden. ([mehr Infos hier](https://github.com/cjhutto/vaderSentiment#about-the-scoring))\n",
    "\n",
    "Aus den 10 angezeigten Bewertungen ergeben sich deutliche Unterschiede zwischen den sehr positiven (Rating 5) und sehr negativen (Rating 1) Bewertungen.\n",
    "- Die'compound'-Werte sind negativ für die negativen Bewertungen bzw. positiv für die positive Bewertungen.\n",
    "- Die 'neg'-Werte liegen bei ~ 20% für die negativen Bewertungen bzw. 0% für die positiven Bewertungen.\n",
    "- Die 'pos'-Werte sind höher für die positiven Bewertungen als für die negativen.\n",
    "- Die 'neu'-Werte sind höher für die positiven Bewertungen als für die negativen, aber der Unterschied ist nicht groß.\n",
    "\n",
    "Im Allgemeinen stimmen die Werte der Sentimenanalyse mit den Ratings der Bewertungen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentimentanalyse mit TextBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob_de import TextBlobDE as TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaffee_bewertungen[\"polarity\"] = kaffee_bewertungen.apply(lambda x: TextBlob(x[\"review\"]).sentiment.polarity, axis=1)\n",
    "kaffee_bewertungen[\"subjectivity\"] = kaffee_bewertungen.apply(lambda x: TextBlob(x[\"review\"]).sentiment.subjectivity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>neu</th>\n",
       "      <th>compound</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>5</td>\n",
       "      <td>Ich bevorzuge Gepa Kaffee, da er FairTrade pro...</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.911</td>\n",
       "      <td>-0.6688</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>5</td>\n",
       "      <td>Kaffee genießen mit gutem Gefühl: besonders zu...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>5</td>\n",
       "      <td>Ich habe einen magenfreundlichen, aber koffein...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>5</td>\n",
       "      <td>Bei Gepa unterstützt man bei jedem Kauf ein we...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>5</td>\n",
       "      <td>Schon lange gehört Kaffee von Gepa zum festen ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review    neg    pos  \\\n",
       "459       5  Ich bevorzuge Gepa Kaffee, da er FairTrade pro...  0.089  0.000   \n",
       "460       5  Kaffee genießen mit gutem Gefühl: besonders zu...  0.000  0.000   \n",
       "461       5  Ich habe einen magenfreundlichen, aber koffein...  0.000  0.000   \n",
       "462       5  Bei Gepa unterstützt man bei jedem Kauf ein we...  0.000  0.000   \n",
       "463       5  Schon lange gehört Kaffee von Gepa zum festen ...  0.000  0.126   \n",
       "\n",
       "       neu  compound  polarity  subjectivity  \n",
       "459  0.911   -0.6688  0.333333      0.166667  \n",
       "460  1.000    0.0000  0.150000      0.000000  \n",
       "461  1.000    0.0000  0.850000      0.000000  \n",
       "462  1.000    0.0000  0.850000      0.000000  \n",
       "463  0.874    0.7351  0.400000      0.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaffee_bewertungen.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>neu</th>\n",
       "      <th>compound</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>Naturland gibt auf ihrer Internetseite an, das...</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.773</td>\n",
       "      <td>-0.9485</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>Ich nutze von REWE Bio Röstkaffee Fairtrade di...</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.755</td>\n",
       "      <td>-0.4389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>Naturland von Rewe</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1</td>\n",
       "      <td>…denn auf der Website wird ausführlich erläute...</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.865</td>\n",
       "      <td>-0.9485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>Aus einigen Testberichten verschiedener Intern...</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.9136</td>\n",
       "      <td>-0.550000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>5</td>\n",
       "      <td>Wir haben in der Vergangenheit den SCHIRMER bi...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5</td>\n",
       "      <td>Bio drauf und bio drin, genau so wie es sein s...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>5</td>\n",
       "      <td>das ist in sehr Geschmackvoller Kaffee,den ich...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>5</td>\n",
       "      <td>Kundenbetreuung: 5 Sterne</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>5</td>\n",
       "      <td>Schon lange gehört Kaffee von Gepa zum festen ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>464 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review    neg    pos  \\\n",
       "38        1  Naturland gibt auf ihrer Internetseite an, das...  0.227  0.000   \n",
       "36        1  Ich nutze von REWE Bio Röstkaffee Fairtrade di...  0.139  0.106   \n",
       "37        1                                 Naturland von Rewe  0.000  0.000   \n",
       "75        1  …denn auf der Website wird ausführlich erläute...  0.135  0.000   \n",
       "102       1  Aus einigen Testberichten verschiedener Intern...  0.206  0.000   \n",
       "..      ...                                                ...    ...    ...   \n",
       "165       5  Wir haben in der Vergangenheit den SCHIRMER bi...  0.000  0.032   \n",
       "164       5  Bio drauf und bio drin, genau so wie es sein s...  0.000  0.000   \n",
       "163       5  das ist in sehr Geschmackvoller Kaffee,den ich...  0.000  0.078   \n",
       "137       5                          Kundenbetreuung: 5 Sterne  0.000  0.000   \n",
       "463       5  Schon lange gehört Kaffee von Gepa zum festen ...  0.000  0.126   \n",
       "\n",
       "       neu  compound  polarity  subjectivity  \n",
       "38   0.773   -0.9485  0.470000      0.000000  \n",
       "36   0.755   -0.4389  0.000000      0.000000  \n",
       "37   1.000    0.0000  0.000000      0.000000  \n",
       "75   0.865   -0.9485  0.000000      0.142857  \n",
       "102  0.794   -0.9136 -0.550000      0.000000  \n",
       "..     ...       ...       ...           ...  \n",
       "165  0.968    0.3182  0.340000      0.000000  \n",
       "164  1.000    0.0000  0.166667      0.000000  \n",
       "163  0.922    0.3182  0.075000      0.000000  \n",
       "137  1.000    0.0000  0.000000      0.000000  \n",
       "463  0.874    0.7351  0.400000      0.000000  \n",
       "\n",
       "[464 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaffee_bewertungen.sort_values(by=\"rating\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Der Wert **'polarity'**  liegt zwischen [-1,1] und gibt an wie negativ/positiv eine Bewertung ist.\n",
    "- Der Wert **'subjectivity'** liegt zwischen [0,1] und gibt an wie objektiv/subjektiv eine Bewertung ist.\n",
    "\n",
    "Aus den 10 angezeigten Bewertungen geht hervor, dass die 'polarity'-Werte der negativen Bewertungen negativ bzw. 0, während die der positiven Bewertungen positiv sind. Die 'subjectivity'-Werte sind 0 sowohl für die positive als auch für negative Bewertungen, was bedeutet, dass die Bewertungen objektiv sind (oder dass das Modell nicht gut funktionert). Im Allgemeinen stimmen die Werte der Sentimenanalyse mit den Ratings der Bewertungen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SpaCy + SentiWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zuerst: pip install spacy-sentiws\n",
    "import spacy\n",
    "from spacy_sentiws import spaCySentiWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Der SentimentWortschatz, oder kurz SentiWS, ist eine öffentlich verfügbare deutschsprachige Ressource für die Sentiment Analyse, Opinion Mining und ähnliche Zwecke. Dabei werden für enthaltene Wörter die positive und negative Polarität im Intervall [-1; 1] angegeben, sowie deren Wortart und (falls anwendbar) Flexionsvarianten. Die aktuelle Version des SentiWS enthält ungefähr 1.650 positive und 1.800 negative Grundformen, so dass, inklusive der verschiedenen Flexionsformen, insgesamt etwa 16.000 positive und 18.000 negative Wortformen enthalten sind. SentiWS enthält nicht nur Adjektive und Adverbien, sondern auch Nomen und Verben die Träger von Sentiment sind.\" ([Quelle](https://wortschatz.uni-leipzig.de/de/download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "from spacy_sentiws.senti_ws_wrapper import SentiWSWrapper\n",
    "\n",
    "\n",
    "class spaCySentiWS(object):\n",
    "    def __init__(self, sentiws_path):\n",
    "        self.sentiws = SentiWSWrapper(sentiws_path=sentiws_path)\n",
    "        Token.set_extension(\"sentiws\", getter=self.get_sentiment, force=True)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        for token in doc:\n",
    "            token._.sentiws = self.get_sentiment(token)\n",
    "        return doc\n",
    "\n",
    "    def get_sentiment(self, token):\n",
    "        return self.sentiws.determine(token.text, pos_universal_google=token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mir, None, PRON\n",
      "schmeckt, None, VERB\n",
      "dieser, None, DET\n",
      "Kaffee, None, NOUN\n",
      "super, 0.5012, ADJ\n",
      "., None, PUNCT\n",
      "Ich, None, PRON\n",
      "trinke, None, VERB\n",
      "ihn, None, PRON\n",
      "bewusst, None, ADJ\n",
      "und, None, CCONJ\n",
      "mit, None, ADP\n",
      "Genuss, 0.0701, NOUN\n",
      "., None, PUNCT\n"
     ]
    }
   ],
   "source": [
    "# einer der drei SpaCy Modelle für Deutsch laden. 'md' = medium\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "sentiws = spaCySentiWS(sentiws_path=\"./spacy_sentiws/SentiWS_v2.0\")\n",
    "nlp.add_pipe(sentiws)\n",
    "\n",
    "# auf einen Beispielsatz testen\n",
    "doc = nlp(\"Mir schmeckt dieser Kaffee super. Ich trinke ihn bewusst und mit Genuss.\")\n",
    "\n",
    "# jedes Wort im Beispielsatz tokenisieren und ihm das Wert aus dem Wortschatz zuweisen\n",
    "for token in doc:\n",
    "    print(\"{}, {}, {}\".format(token.text, token._.sentiws, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwei \"Sentiment\"-Wörter wurden bewertet: *super* als positiv (0.5) und *Genuss* als neutral (0.07). Der Satz ist aber sehr positiv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SpaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inhalt</th>\n",
       "      <th>weiter_empfehlung</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>221716</th>\n",
       "      <td>Liebe Studenten und Studieninteressierte,  an ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221717</th>\n",
       "      <td>Studiert es besser nicht! Nach 2 Semestern war...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221718</th>\n",
       "      <td>Habe den besagten Studiengang zwei Semester in...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221719</th>\n",
       "      <td>Ja, ja, die liebe Hochschule Rhein Waal. Wenn ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221720</th>\n",
       "      <td>Es ist gut zu sehen, dass man das soziale mit ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   inhalt  weiter_empfehlung\n",
       "221716  Liebe Studenten und Studieninteressierte,  an ...               True\n",
       "221717  Studiert es besser nicht! Nach 2 Semestern war...              False\n",
       "221718  Habe den besagten Studiengang zwei Semester in...              False\n",
       "221719  Ja, ja, die liebe Hochschule Rhein Waal. Wenn ...              False\n",
       "221720  Es ist gut zu sehen, dass man das soziale mit ...               True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_bewertungen = pd.read_csv(\"studycheckv2.csv\")\n",
    "uni_bewertungen = uni_bewertungen.drop([\"id\", \"url\"], axis=1)\n",
    "uni_bewertungen.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inhalt</th>\n",
       "      <th>weiter_empfehlung</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>221716</th>\n",
       "      <td>Liebe Studenten und Studieninteressierte,  an ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221717</th>\n",
       "      <td>Studiert es besser nicht! Nach 2 Semestern war...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221718</th>\n",
       "      <td>Habe den besagten Studiengang zwei Semester in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221719</th>\n",
       "      <td>Ja, ja, die liebe Hochschule Rhein Waal. Wenn ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221720</th>\n",
       "      <td>Es ist gut zu sehen, dass man das soziale mit ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   inhalt  weiter_empfehlung\n",
       "221716  Liebe Studenten und Studieninteressierte,  an ...                  1\n",
       "221717  Studiert es besser nicht! Nach 2 Semestern war...                  0\n",
       "221718  Habe den besagten Studiengang zwei Semester in...                  0\n",
       "221719  Ja, ja, die liebe Hochschule Rhein Waal. Wenn ...                  0\n",
       "221720  Es ist gut zu sehen, dass man das soziale mit ...                  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# die Weiterempfehlungswerte numerisch enkodieren\n",
    "\n",
    "uni_bewertungen[\"weiter_empfehlung\"] = uni_bewertungen[\"weiter_empfehlung\"].astype(int)\n",
    "uni_bewertungen.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    199287\n",
       "0     22434\n",
       "Name: weiter_empfehlung, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_bewertungen[\"weiter_empfehlung\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Dataset gibt es 199287 positive und 22434 negative Bewertungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stoppwörter entfernen \n",
    "\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.de import German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de import German\n",
    "parser = German()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Die',\n",
       " 'Ausrüstung',\n",
       " 'in',\n",
       " 'den',\n",
       " 'Laboren',\n",
       " 'ist',\n",
       " 'sehr',\n",
       " 'modern',\n",
       " 'und',\n",
       " 'neu',\n",
       " 'sowie',\n",
       " 'technisch',\n",
       " 'hochwertig',\n",
       " '.',\n",
       " 'Die',\n",
       " 'Anleitungen',\n",
       " 'im',\n",
       " 'Labor',\n",
       " 'sind',\n",
       " 'ebenfalls',\n",
       " 'gut',\n",
       " '(',\n",
       " 'u.a',\n",
       " '.',\n",
       " 'da',\n",
       " 'man',\n",
       " 'sich',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'die',\n",
       " 'Kurse',\n",
       " 'vorbereiten',\n",
       " 'muss',\n",
       " ')',\n",
       " '.',\n",
       " 'Zwar',\n",
       " 'sind',\n",
       " 'alle',\n",
       " 'Vorlesungen',\n",
       " 'auf',\n",
       " 'englisch',\n",
       " ',',\n",
       " 'jedoch',\n",
       " 'kann',\n",
       " 'man',\n",
       " 'sich',\n",
       " 'schnell',\n",
       " 'reinhören',\n",
       " '.',\n",
       " 'Doch',\n",
       " 'wenn',\n",
       " 'man',\n",
       " 'Fragen',\n",
       " 'hat',\n",
       " 'so',\n",
       " 'stehen',\n",
       " 'die',\n",
       " 'Professoren',\n",
       " 'nach',\n",
       " 'den',\n",
       " 'Vorlesungen',\n",
       " 'auch',\n",
       " 'gerne',\n",
       " 'auf',\n",
       " 'deutsch',\n",
       " 'mit',\n",
       " 'Rat',\n",
       " 'zurseite',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# erste Bewertung tokenisieren (TEST)\n",
    "nltk.word_tokenize(uni_bewertungen[\"inhalt\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'inhalt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-af443e17befb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minhalt_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muni_bewertungen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minhalt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minhalt_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minhalt_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minhalt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0minhalt_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0minhalt_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minhalt_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'inhalt'"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "\n",
    "inhalt_df = uni_bewertungen.inhalt\n",
    "inhalt_corpus = inhalt_df.inhalt.str.cat(sep = '\\n')\n",
    "inhalt_corpus[0:1000]\n",
    "inhalt_text = nltk.Text(inhalt_df)\n",
    "#positive_tokens = [stemmer.stem(x) for x in positive_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>inhalt</th>\n",
       "      <th>weiter_empfehlung</th>\n",
       "      <th>inhalt_tokenized</th>\n",
       "      <th>inhalt_laenge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41169</td>\n",
       "      <td>https://www.studycheck.de/studium/bioingenieur...</td>\n",
       "      <td>Die Ausrüstung in den Laboren ist sehr modern ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[Die, Ausrüstung, in, den, Laboren, ist, sehr,...</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41171</td>\n",
       "      <td>https://www.studycheck.de/studium/internationa...</td>\n",
       "      <td>Ein Studium in International Business an der H...</td>\n",
       "      <td>True</td>\n",
       "      <td>[Ein, Studium, in, International, Business, an...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41173</td>\n",
       "      <td>https://www.studycheck.de/studium/internationa...</td>\n",
       "      <td>Allen voran: Der Campus - zumindest die baulic...</td>\n",
       "      <td>False</td>\n",
       "      <td>[Allen, voran, :, Der, Campus, -, zumindest, d...</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41175</td>\n",
       "      <td>https://www.studycheck.de/studium/medieninform...</td>\n",
       "      <td>Das schlechte zuerst: das Campusleben wird dad...</td>\n",
       "      <td>True</td>\n",
       "      <td>[Das, schlechte, zuerst, :, das, Campusleben, ...</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41177</td>\n",
       "      <td>https://www.studycheck.de/studium/arbeits-und-...</td>\n",
       "      <td>Da es sich um eine neuere HS handelt, herrsche...</td>\n",
       "      <td>True</td>\n",
       "      <td>[Da, es, sich, um, eine, neuere, HS, handelt, ...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                                url  \\\n",
       "0  41169  https://www.studycheck.de/studium/bioingenieur...   \n",
       "1  41171  https://www.studycheck.de/studium/internationa...   \n",
       "2  41173  https://www.studycheck.de/studium/internationa...   \n",
       "3  41175  https://www.studycheck.de/studium/medieninform...   \n",
       "4  41177  https://www.studycheck.de/studium/arbeits-und-...   \n",
       "\n",
       "                                              inhalt  weiter_empfehlung  \\\n",
       "0  Die Ausrüstung in den Laboren ist sehr modern ...               True   \n",
       "1  Ein Studium in International Business an der H...               True   \n",
       "2  Allen voran: Der Campus - zumindest die baulic...              False   \n",
       "3  Das schlechte zuerst: das Campusleben wird dad...               True   \n",
       "4  Da es sich um eine neuere HS handelt, herrsche...               True   \n",
       "\n",
       "                                    inhalt_tokenized  inhalt_laenge  \n",
       "0  [Die, Ausrüstung, in, den, Laboren, ist, sehr,...             69  \n",
       "1  [Ein, Studium, in, International, Business, an...             72  \n",
       "2  [Allen, voran, :, Der, Campus, -, zumindest, d...            434  \n",
       "3  [Das, schlechte, zuerst, :, das, Campusleben, ...            601  \n",
       "4  [Da, es, sich, um, eine, neuere, HS, handelt, ...            159  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv Datei in dataframe konvertieren\n",
    "uni_bewertungen_df = pd.read_csv(\"studycheckv2.csv\")\n",
    "\n",
    "uni_bewertungen_df['inhalt_tokenized'] = uni_bewertungen_df.apply(lambda row: nltk.word_tokenize(row['inhalt']), axis=1)\n",
    "uni_bewertungen_df['inhalt_laenge'] = uni_bewertungen_df.apply(lambda row: len(row['inhalt_tokenized']), axis=1)\n",
    "uni_bewertungen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inhalt</th>\n",
       "      <th>weiter_empfehlung</th>\n",
       "      <th>inhalt_tokenized</th>\n",
       "      <th>inhalt_laenge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Die Ausrüstung in den Laboren ist sehr modern ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Die, Ausrüstung, in, den, Laboren, ist, sehr,...</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ein Studium in International Business an der H...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Ein, Studium, in, International, Business, an...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allen voran: Der Campus - zumindest die baulic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Allen, voran, :, Der, Campus, -, zumindest, d...</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Das schlechte zuerst: das Campusleben wird dad...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Das, schlechte, zuerst, :, das, Campusleben, ...</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Da es sich um eine neuere HS handelt, herrsche...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Da, es, sich, um, eine, neuere, HS, handelt, ...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inhalt  weiter_empfehlung  \\\n",
       "0  Die Ausrüstung in den Laboren ist sehr modern ...                  1   \n",
       "1  Ein Studium in International Business an der H...                  1   \n",
       "2  Allen voran: Der Campus - zumindest die baulic...                  0   \n",
       "3  Das schlechte zuerst: das Campusleben wird dad...                  1   \n",
       "4  Da es sich um eine neuere HS handelt, herrsche...                  1   \n",
       "\n",
       "                                    inhalt_tokenized  inhalt_laenge  \n",
       "0  [Die, Ausrüstung, in, den, Laboren, ist, sehr,...             69  \n",
       "1  [Ein, Studium, in, International, Business, an...             72  \n",
       "2  [Allen, voran, :, Der, Campus, -, zumindest, d...            434  \n",
       "3  [Das, schlechte, zuerst, :, das, Campusleben, ...            601  \n",
       "4  [Da, es, sich, um, eine, neuere, HS, handelt, ...            159  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_bewertungen_df[\"weiter_empfehlung\"] = uni_bewertungen_df[\"weiter_empfehlung\"].astype(int)\n",
    "uni_bewertungen_df = uni_bewertungen_df.drop([\"id\", \"url\"], axis=1)\n",
    "uni_bewertungen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt(r'studycheckv2.txt', uni_bewertungen_df.weiter_empfehlung, fmt='%d', delimiter=\"\\t\", header=\"inhalt\\tweiter_empfehlung\\tinhalt_tokenized\\tinhalt_laenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfile = open('unibewertungen.txt', 'a')\n",
    "tfile.write(uni_bewertungen_df.to_string())\n",
    "tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sonderzeichen/Stoppwörter entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punct = string.punctuation\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "vect = CountVectorizer(stop_words = german_stop_words) # Now use this in your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(x):\n",
    "    token = x.split()\n",
    "    return ' '.join([w for w in token if not w in german_stop_words])\n",
    "\n",
    "\n",
    "uni_bewertungen_df['removed_stop_word']  = uni_bewertungen_df['inhalt'].apply(stop_word_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hallo, geht's dir?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_word_removal(\"Hallo, wie geht's dir?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer = vect)\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = uni_bewertungen_df[\"inhalt\"]\n",
    "y = uni_bewertungen_df[\"weiter_empfehlung\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((177376,), (44345,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6168      Da\\n die Hdpk eine private Hochschule ist, kos...\n",
       "112126    Das Studium an der ISM bringt Spaß, die LehrIn...\n",
       "159706    Meine Studium würde ich mit einer Weltreise gl...\n",
       "152989    Ich bin schon Erzieherin und bringe daher scho...\n",
       "88063     Ich habe gelernt mich kritisch mit vielen Vers...\n",
       "                                ...                        \n",
       "119879    Die Erfahrungen waren sehr unterschiedlich. Es...\n",
       "103694    Ideenwelten werden real.\\r\\nViel mehr kann ich...\n",
       "131932    Ich finde, dass man zwar in kurzer Zeit viel S...\n",
       "146867    Ich finde besonders super, dass die Uni durch ...\n",
       "121958    Ich habe meinen Bachelor an einer anderen Uni ...\n",
       "Name: inhalt, Length: 177376, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([(\"tfidf\", tfidf), (\"clf\", classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CountVectorizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-4f66baab8f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'CountVectorizer' object is not callable"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-ddc3f623c4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline = Pipeline([\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'vect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopWords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopWords' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-f7a4cc69b9a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                    \"be removed in 0.24.\")\n\u001b[1;32m   1897\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m                 \"string object received.\")\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e7622b950e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-430e012b2078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# das Modell auf ein Beispiel testen\n",
    "\n",
    "clf.predict([\"Das ist eine tolle Uni mit netten Professoren und interessanten Lehrveranstaltungen!\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
